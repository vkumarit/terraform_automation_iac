Terraform 
---------

# Azure Authentication #

>>> Server-side/backend Alternatives for Terraform Authentication <<<

I. Using Service Principal secrets (tenant_id, subscription_id, client_id, client_secret)

A) For first time/step, also when we exit the session with exported vars and
   starting new session. The typical secure workflow is:

> a) > Create an App Registration (Service Principal) in Azure Portal
  - Register an application in Azure AD.
  - Note the Client ID, Tenant ID.
  - Generate a Client Secret.
  - Assign the Service Principal the necessary roles (e.g., Contributor) 
    on your subscription or resource group.
  - generate Personam Access Token from azure organization to setup agent on vm.
  
  b) > Create Service Principal using Azure AD CLI
  - Log into Azure (Cloud Shell or local machine) and do:
    $ az ad sp create-for-rbac --name "terraform-ec2-sp" --role Contributor \
        --scopes /subscriptions/<SUBSCRIPTION_ID> --sdk-auth
    This prints JSON including clientId (AppId), clientSecret, tenantId, subscriptionId. Store them 
    securely and use on the EC2 instance.
(SKIP IF APP REGISTRATION DONE)
  * For key vault access assign 'get' access to this service principal when needed to fetch secrets from      key vault

## Azure authentication from outside Azure (like terraform on EC2) ##

> b) > Export the Service Principal credentials as environment variables on our EC2 instance, do bash
  $ export ARM_CLIENT_ID="<client-id-from-app-registration>"
  $ export ARM_CLIENT_SECRET="<client-secret-from-app-registration>"
  $ export ARM_SUBSCRIPTION_ID="<our-subscription-id>"
  $ export ARM_TENANT_ID="<tenant-id-from-app-registration>"
  This enables Terraform to authenticate securely without embedding secrets in code.
  Terraform will read the ARM_* env vars and authenticate, can use below provider block at minimum:
	provider "azurerm" {
	  features {}
	}

  Change the exported env vars as
    1) To see all env vars,
       $ env or printenv
    2) filter by pattern
       $ printenv | grep ARM_* 
    3) To check single variable,
       $ echo $ARM_CLIENT_ID
    4) To reset a variable export it again as
       $ export ARM_CLIENT_ID="new-client-id"
    5) Remove from current shell,
       $ unset ARM_CLIENT_SECRET

  Also used in dev, test environment.

or,

  b) > Azure CLI Interactive or Service Principal Login:
  - Run 'az login' (interactive or with SP credentials) on your EC2 instance before running Terraform.
    $ az login \
        --service-principal \
        --username <clientId> \
        --password <clientSecret> \
        --tenant <tenantId>
    
    If getting error 'No subscriptions found for <client-Id>.`, go to subscription > IAM > 
    Add role assignment > Role > Priviledged admin. roles > select 'Owner'/'contributor' > Members > User,         group, or service principal > Select members > 'az-classic-app' (name of app registered)> assigns role 
    'owner' at scope 'This resource'.

  - Verify the subscription used by terraform.
    $ az account show
    
  - Configure Terraform to use AzureRM provider
  
    Terraform >=4 requires subscription_id for azure authentication, so
    i) Export subscription id to env vars,
       $ export ARM_SUBSCRIPTION_ID="<our-subscription-id>" 
       
       terraform {
         required_providers {
           azurerm = {
             source  = "hashicorp/azurerm"
             version = "~> 4.0"
          }
         }
       }
    
       provider "azurerm" {
         features {}
       }
    
    ii) Hardcode subscription id as below without exporting to env vars,
       provider "azurerm" {
         features         {}
         subscription_id  = "<-subscription_id->"
       }
    
    iii) Else, after exporting id to env vars, explicitly reference as a variable
       variable "subscription_id" {
          description = "Azure subscription ID"
          type        = string
          default     = null  # Falls back to env var
       }

       provider "azurerm" {
         features         {}
         subscription_id  = var.subscription_id
       }

    

    When the Azure CLI is logged in with a user? or service principal, AzureRM can automatically use the SP     credentials.

  - Terraform picks up the authentication context from the Azure CLI session.
  Use Case: Manual runs or environments where you can run CLI commands. Can also be used subsequently   afterwards initial setup.

> c) > Run terraform init, plan -out ('-out' so terraform guarantee to act exactly as was shown in plan phase),
       and apply commands EC2 instance. Terraform will use these environment variables (or az login context)
       to authenticate and create our Azure resources including key vault.

or,
     > Use .tfvars file 
  - In variable.tf, mark less-sensitive secret as 'sensitive = true' and keep these less-sensitive secrets
    in .tfvars file 
  - Don't add and push it to version control.
  - Env - test/dev 
  - Not suitable to keep secrets when production env.

or, if

## Azure authentication from inside Azure Resources(VM, Container, DevOps pipeline) ##
 
  b) > Use 'Managed Identity' by adding below to provider.tf. No secrets need to be provided at all,

  provider "azurerm" {
  features = {}
  use_msi  = true
  }

> c) > Run terraform init plan apply commands from our Azure (VM, Container, DevOps pipeline)
  Terraform will use these environment variables to authenticate 
  and create our Azure resources including key vault.


B) For subsequent use, after first step / Authentication is done, switch to using dynamic
   secret fetching from vault
  - Store credentials/secrets in Azure Key Vault or HashiCorp Vault (or other secret managers like of CI/CD platforms as Jenkins).
  - Use 'azurerm_key_vault_secret' data source in Terraform to fetch secrets dynamically during future runs,
    avoiding hardcoding or manually exporting secrets.
  - setup service principal access to key vault secrets with read (get) permissions, 
    for strong access control or limited service principal with permission to read (get) those secrets.

> a) > Go to Azure Dashboard > key vault > Add Secrets
  and/or 
  Add Secrets to Key Vault using azure cli as below:
  az keyvault secret set --vault-name MyKeyVault --name "client-id" --value "<your-client-id>"
  az keyvault secret set --vault-name MyKeyVault --name "client-secret" --value "<your-client-secret>"
  az keyvault secret set --vault-name MyKeyVault --name "subscription-id" --value "<your-subscription-id>"
  az keyvault secret set --vault-name MyKeyVault --name "tenant-id" --value "<your-tenant-id>"

III. Azure AD Workload Identity Federation (OIDC)
  - Federate your CI/CD platform (GitHub Actions, Jenkins, etc.) with Azure AD using OpenID Connect.
  - The CI/CD pipeline presents an identity token to Azure AD, which is mapped to a federated credential.
  - Terraform authenticates with Azure using this token, not secrets.
  - use Azure federated credentials to assign RBAC to the identity group.
  Use Case: Secure CI/CD pipelines, especially for GitHub Actions, Azure DevOps, or other cloud-native   workflows.
  ?? How to generate and use federated token? bootstrap initial run or subsequent usage? steps ?

IV. Use Azure Key Vault Access with Short-Lived Tokens

  - Use a trusted service to generate short-lived access tokens or SAS tokens for Key Vault.
  - Inject these tokens into your environment for each Terraform run.
  Use Case: Highly secure, automated environments with strict secret rotation policies.
  ?? How steps


>>> Frontend Alternatives for Terraform Authentication <<<

V. CI/CD Pipeline Secrets Storage  
  - Use the built-in secrets management features of CI/CD tools 
  - such as GitHub Actions, Azure DevOps, or Jenkins
  - store and encrypt secrets then inject them as environment variables into build and deployment jobs.
  - Env - prod.

 > How to configure CI/CD or automation agents to inject Service Principal secrets as environment 
   variables securely for terraform?
  
  a) With Azure DevOps: 
     In classic pipelines, environment variables mapped in one task do not persist to other tasks.
       1) Secret Variables:
          > Go to Pipelines > Select pipeline > Edit.
          > Click on the Variables tab.
          > Add a new variable (e.g., ARM_CLIENT_SECRET, ARM_CLIENT_ID, ARM_TENANT_ID, ARM_SUBSCRIPTION_ID).
          > Enter value > check Keep this value secret.
          > Save pipeline.
          > Classic Pipeline - Go to Environment Variables, map the secret variable to an environment 
            variable (e.g., variable name as ARM_CLIENT_SECRET and value as $(ARM_CLIENT_SECRET), 
            so Terraform on vm picks them up as ARM_* variables during that tasks' runtime. The pipeline tasks             we do this with are below, 
            i) a Bash or PowerShell script task, in a script body add below and run it,
               
               terraform init
               terraform plan -out=tfplan
               terraform apply tfplan
 
            ii) a Terraform task (by microsoft), configured with Environment Variables as mentioned. 

          > YAML Pipeline - In YAML file, reference the variable as $(ARM_CLIENT_SECRET). 
            To use it in a script, map it to an environment variable as:

            steps:
            - script: |
                echo "Secret value is set"
                # Terraform steps here
              env:
                ARM_CLIENT_SECRET: $(ARM_CLIENT_SECRET)

​
       2) Variable Groups (recommended for reuse, used in production):
          > Go to Azure DevOps project > Pipelines > Library > Click + Variable group.
          > Enter a name for the group (e.g., prod-terraform-vars). 
          > Add variables (e.g., ARM_CLIENT_SECRET, ARM_CLIENT_ID, ARM_TENANT_ID, ARM_SUBSCRIPTION_ID) and 
            check the secret checkbox for sensitive values.
          > Save the variable group.
          > Classic Pipeline - Edit pipeline > Variables tab > Click Link variable group 
            > select prod-terraform-vars group > Save pipeline. 
              In pipeline tasks, under Environment Variables, map the secret variable to an                                   environment variable (e.g., var as ARM_CLIENT_SECRET and value as $(ARM_CLIENT_SECRET),
              so Terraform picks them up as ARM_* variables during that tasks' runtime, The pipeline tasks                    we do this with are below,
              i) a Bash or PowerShell script task, in a script body add below and run it,
                
                terraform init
                terraform plan -out=tfplan
                terraform apply tfplan
 
              ii) a Terraform task (by microsoft), configured with Environment Variables as mentioned. 

          > YAML Pipeline - In 'azure-pipelines.yaml' file, reference the variable group like this:
            
            variables:
            - group: prod-terraform-vars

            steps:
            - script: |
                echo "Using secret ARM_CLIENT_ID" 
                # echoing secrets not recommended
                # Your Terraform steps here
               displayName: Run Terraform

            Secret values will be injected to env and masked in logs and
            Limit access to the variable group and pipeline to authorized users.
            When needed rotate secrets and update variable group.

       3) Variable Groups linked with Key Vault:
          Recommended for production - Store service principal secrets (client secret) and other sensitive                values in Azure Key Vault. Expose them to Azure DevOps pipeline via a variable group linked to 
          that Key Vault.
          i) Store secrets in Azure Key Vault: 
             a) Azure portal > Create/choose Key Vault in prod env, and 
             b) Enable RBAC or vault access policies, 
             c) Restrict access so only few admins and Azure DevOps service principal can read secrets.
             d) Add service principal secrets to Key Vault as 
                tf-ARM-CLIENT-ID(non-prod)/tf-ARM-CLIENT-ID-prod etc.
          ii) Link Key Vault to Azure DevOps (variable group):
             a) Create service connection - 
                Go to Azure DevOps > project > Project settings > Service connections > Create an “Azure                        Resource Manager” service connection that has access to production Key Vault 
                (subscription + proper role, e.g., Key Vault Secrets User / Reader).
             b) Create a variable group linked to Key Vault -
                Go to Pipelines > Library > + Variable group > Name it as 'prod-terraform-kv' 
                > Enable “Link secrets from an Azure Key Vault as variables” > Choose the ARM service                           connection we created > select production Key Vault > Add secrets from vault, 
                like (tf-ARM-CLIENT-ID, etc.) > Save the variable group.
                The actual values stay only in Key Vault, Azure DevOps just references them.
          iii) Connect variable group to the production pipeline -
             a) Open production classic pipeline > click Edit > Variables tab > click “Link variable group”
                > Select 'prod-terraform-kv' variable group > save pipeline.
                > In pipeline tasks, under Environment Variables, map the secret variable to an                                   environment variable (e.g., var as ARM_CLIENT_SECRET and value as $(tf-ARM-CLIENT-SECRET),
                  so Terraform picks them up as ARM_* variables during that tasks' runtime. The pipeline tasks                    we do this with are below,
                  i) a Bash or PowerShell script task, in a script body add below and run it,
                
                     terraform init
                     terraform plan -out=tfplan
                     terraform apply tfplan
 
                  ii) Or, a Terraform task (by microsoft), configured with Environment Variables as mentioned.

             b) YAML Pipeline - In 'azure-pipelines.yaml' file, reference the variable group like this:
            
                variables:
                - group: prod-terraform-vars

                steps:
                - script: |
                    echo "Using secret ARM_CLIENT_ID" 
                    # echoing secrets not recommended
                    # Your Terraform steps here
                  displayName: Run Terraform

                Secret values will be injected to env and masked in logs and
                Limit access to the variable group and pipeline to authorized users.
                When needed rotate secrets from Key Vault.
       
       4)  Use a task such as AzureCLI@2 or the Terraform task - 
           that is bound to an Azure service connection and exports SP details to env vars 
           (via addSpnToEnvironment or similar) before running terraform init/plan/apply. 
           Azure Resource Manager (ARM) service connection - Grants Azure DevOps pipelines the 
           ability to authenticate and manage Azure resources, and it's the connection used by 
           tasks like AzureCLI@2, Azure Resource Group Deployment, and Terraform when set to use
           Azure authentication.
           i) Using AzureCLI@2 - 
                a) Install azure cli on ec2 instance once, manual az login is not required.
                b) Create task AzureCLI@2 in classic pipeline and select ARM service connection (SP) 
                   we created as it leverages the credentials from the ARM service connection 
                   (which is based on service principal) to authenticate and set the environment 
                   variables for pipeline tasks.
                c) Script Type: Choose "Bash" or "shell" (for Linux agents) or 
                   "PowerShell" (for Windows agents).
                d) Script Location: Choose "Inline script" or "Script file"
                   The environment variables ($servicePrincipalId, $servicePrincipalKey, $tenantId) are                            available for script. We use these variables in Bash script as,
                   
                   # Export to ARM_* env var for Terraform from inline script, 
                   # then do terraform init plan apply
                   export ARM_CLIENT_ID=$servicePrincipalId
                   export ARM_CLIENT_SECRET=$servicePrincipalKey
                   export ARM_TENANT_ID=$tenantId
                   export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv)
                   
                   These environment variables are only available for the duration of the pipeline job and are                     not persisted or exposed outside the job.
                e) Script: Enter Azure CLI commands (for example, 'az account show' or 'az group list') and                        save pipeline.
           ii) Using Terraform task -
                a) create terraform task, select ARM service connection, set the Terraform task to use that                        service connection, 
                   - enable “Use Env Vars for Authentication” in the Terraform task UI, the task will                      automatically set ARM_CLIENT_ID, ARM_CLIENT_SECRET (or OIDC token), ARM_TENANT_ID, and                          ARM_SUBSCRIPTION_ID for that Terraform task based on the selected service connection.
                     Or,
                   - 'Use Entra ID for Authentication' option is when we want to authenticate using Azure AD                         (Entra ID) credentials, such as managed identities or user-based authentication. 

                   For service principals, stick with "Use Env Vars for Authentication". Means, the needed SP                      details are automatically injected into the environment for that task, allowing Terraform 
                   to authenticate. 
                     (Alternatively, we add variable to env var option under task)

-------------------------------------------------------------------------------------------------------------
Terraform Commands:
-------------------
(Part of Bootstrap plan- manually create resources at azure then shift to azure devops cicd pipeline)

>> After changing data, resource blocks' code etc to refresh statefile free from errors
# Clean state
$ rm -rf .terraform .terraform.lock.hcl terraform.tfstate*
and also,
# Remove local files after moving backened to azure storage remote
$ rm -f errored.tfstate plan.tfplan terraform.tfstate terraform.tfstate.backup


# Fresh start
$ terraform init > validate > plan > apply 

# Always better to preview and lock the exact plan
$ terraform plan -out=plan.tfplan
$ terraform apply plan.tfplan


>> After adding the backened block to move statefile to created storage account, use
# initialize the remote backend
$ terraform init -upgrade
# plan show no changes (verify)
$ terraform plan
# show list of resources on statefile
$ terraform state list
# unlock statefile (if needed)
$ terraform force-unlock

# Check remote state (will receive a json from remote)
$ terraform state pull
# Confirm backened
$ terraform providers


----Start-Error-handling----
If change the name in block after resource type, like changing 'mytfstate' to 'prodmyapp' as
``` resource "azurerm_storage_account" "prodmyapp" {...} ``` and it destroyed existing resources before recreating them, causing a DNS lookup failure for the storage account and backend state access issues.

Errors - Failed to save state, Failed to persist state to backend, Error releasing the state lock, A resource (RG) with the ID already exists needs to be imported into the State - "Error releasing the state lock 
Error message: failed to retrieve lock info: executing request: Head 
"https://prodmyapptfstate01.blob.core.windows.net/mytfstate/terraform.tfstate": dial tcp: lookup prodmyapptfstate01.blob.core.windows.net on 172.31.0.2:53: no such host".

> Follow Steps - 

Summary of recovery sequence - force-unlock > import RG > state rm old entries > init -upgrade > validate and continue.
#Force unlock the state lock if get below like error
$ terraform force-unlock <lock-id-from-error>
Example error case (contains lock id) - "Error: Error acquiring the state lock State locked to player 12345678-abcd-1234-efgh-1234567890ab at rg_name/myaccount/mytfstate/terraform.tfstate"

Skip to next step,

Summary of recovery sequence - Recreate RG, storage account + container > Initialize backend > Import existing resources (RG, SA, SC) > Validate and continue.
#In our case, lock is broken due to storage account deletion, so force-unlock won't work. Terraform already saved recovery state locally, we initialize it manually to backend later after recreation, verify recovery statefile exists,
$ ls -la errored.tfstate

#Recreate infrastructure manually
$ az group create --name myTFResourceGroup --location "Australia East"
az storage account create --name prodmyapptfstate01 --resource-group myTFResourceGroup \
  --location "Australia East" --sku Standard_LRS --kind StorageV2
az storage container create --name mytfstate --account-name prodmyapptfstate01

#Initialize terraform
$ terraform init -upgrade

#Import all existing resources into state
- Import Resource Group
$ terraform import azurerm_resource_group.prodmyapp /subscriptions/2b2f02f7-dde2-47db-974c-47d2182721ae/resourceGroups/myTFResourceGroup
- Import Storage Account  
$ terraform import azurerm_storage_account.prodmyapp /subscriptions/2b2f02f7-dde2-47db-974c-47d2182721ae/resourceGroups/myTFResourceGroup/providers/Microsoft.Storage/storageAccounts/prodmyapptfstate01
- Import Storage Container
$ terraform import azurerm_storage_container.prodmyapp https://prodmyapptfstate01.blob.core.windows.net/mytfstate

#Clean up any remaining old state entries with mytfstate listed
$ terraform state list
azurerm_resource_group.mytfstate
azurerm_storage_account.mytfstate
azurerm_storage_container.mytfstate
then, remove
$ terraform state rm azurerm_resource_group.mytfstate
$ terraform state rm azurerm_storage_account.mytfstate
$ terraform state rm azurerm_storage_container.mytfstate

#Validate and proceed
$ terraform plan
----End-Error-handling----

----Start-Error-handling----
```
Error: Error acquiring the state lock

Error message: state blob is already locked
Lock Info:
  ID:        7d0f5123-6d43-3eac-6acb-352879a60005
  Path:      mytfstate/terraform.tfstate
  Operation: OperationTypePlan
  Who:       cloud-user@terraform-server
  Version:   1.14.4
  Created:   2026-02-13 13:52:19.862423651 +0000 UTC
  Info:      

Terraform acquires a state lock to protect the state from being written
by multiple users at the same time. Please resolve the issue above and try
again. For most commands, you can disable locking with the "-lock=false"
flag, but this is not recommended.
Terraform plan FAILED

```
Go to location/directory where terraform files are stored 
like `/home/cloud-user/terraform_automation_iac/azuretf/simpletf` and 
run bash cmd,
$ terraform force-unlock 7d0f5123-6d43-3eac-6acb-352879a60005

----End-Error-handling----

-------------------------------------------------------------------------------------------------------------
How to use `terraform import` in both **Azure** and **AWS** contexts, either from a **CI/CD pipeline** or **locally**. 

Step-by-step:

1. Understanding `terraform import`

* `terraform import` is used to 'bring existing infrastructure under Terraform management'.
* It DOESN'T generate `.tf` configuration automatically; we must write the resource block yourself.
* It **links an existing resource** in your cloud account to a Terraform resource in your state file.

Syntax:
$ terraform import [options] <RESOURCE_ADDRESS> <RESOURCE_ID>
* `RESOURCE_ADDRESS` → The Terraform resource name in your `.tf` files, e.g., `aws_s3_bucket.my_bucket`.
* `RESOURCE_ID` → The actual ID of the resource in the cloud provider.

2. Using `terraform import` Locally

### **Step-by-step (AWS/Azure example)**

- Create a `.tf` resource block (without applying yet):

resource "aws_s3_bucket" "my_bucket" {
  bucket = "my-existing-bucket"
}

resource "azurerm_resource_group" "my_rg" {
  name     = "existing-rg"
  location = "East US"
}

- Initialize Terraform:
$ terraform init

- Import the resource:
$ terraform import aws_s3_bucket.my_bucket my-existing-bucket
$ terraform import azurerm_resource_group.my_rg /subscriptions/<SUBSCRIPTION_ID>/resourceGroups/existing-rg


- Check state:
$ terraform state list
$ terraform state show azurerm_resource_group.my_rg

- Run `terraform plan` > differences > update `.tf` file > match actual resource attributes.

3. Using `terraform import` in a Pipeline (CI/CD)

If running Terraform in a pipeline (GitHub Actions, Azure DevOps, Jenkins, etc.), we need to **authenticate** with cloud provider and ensure Terraform can write to the state backend.

### **Pipeline Steps Example (AWS)**

```yaml
steps:
  - name: Checkout code
    uses: actions/checkout@v3

  - name: Setup Terraform
    uses: hashicorp/setup-terraform@v2

  - name: Configure AWS credentials
    uses: aws-actions/configure-aws-credentials@v2
    with:
      aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      aws-region: us-east-1

  - name: Terraform Init
    run: terraform init

  - name: Terraform Import
    run: terraform import aws_s3_bucket.my_bucket my-existing-bucket

  - name: Terraform Plan
    run: terraform plan
```

4. **Summary**

* `terraform import` is for linking existing resources to Terraform.
* Syntax is provider-specific but conceptually the same.
* `Write the resource block before importing.` Terraform won’t create `.tf` files for you.
* Check resource IDs carefully.** AWS uses names/ARNs; Azure uses full resource IDs.
* Locally: run `terraform init` > `terraform import` > `terraform plan`.
* In pipelines: authenticate > init > import > plan > apply (optional).
* `Use remote state in pipelines` to prevent state conflicts and also if want the state shared across runs.
* Always run `terraform plan` after import** to sync your config with real resource settings.
* Always verify and adjust the `.tf` config after import.
* Automate carefully** — importing is usually a `one-time operation` per resource, not a repeated pipeline step.

---

# Scenario - *Exactly* the real-world Terraform scenario people hit in production.

* ./ Remote backend **already configured** (S3 / Azure Blob)
* ./ Terraform is running in production
*  X Someone creates a resource **manually in the cloud dashboard**
*  X That resource **does not exist** in your `.tf` files
*  ? What happens next?
---

> Reality in cloud account, Terraform state (remote backend) and Terraform configuration (.tf)

Cloud account:
  - Resource A (Terraform-managed)
  - Resource B (Terraform-managed)
  - Resource C (MANUALLY created)

terraform.tfstate:
  - Resource A
  - Resource B

main.tf:
  - Resource A
  - Resource B

As Terraform NEVER SCANS ACCOUNT, Terraform has NO idea Resource C exists.
It only knows about .tf and statefile. This is called 'configuration drift outside Terraform'.
---

> Now, when pipelines / terraform plan runs
$ terraform plan
# Result:
* ./ No changes
*  X Resource C is ignored with No warning & No error

---
*** Handling in Production (IMPORTANT) ***

## Step-by-step workflow - To bring the manually created resource under Terraform control SAFELY.

-> Step 1: Identify the resource details
From dashboard / CLI, find out Name, ID, Region, Resource group / VPC / subnet, Tags. 
Example: AWS S3 bucket: `prod-logs-bucket`, Azure RG: `prod-rg-extra`.

-> Step 2: Add resource block to `.tf` (NO APPLY) - Examples AWS/Azure:

resource "aws_s3_bucket" "prod_logs" {
  bucket = "prod-logs-bucket"
}

resource "azurerm_resource_group" "prod_extra" {
  name     = "prod-rg-extra"
  location = "East US"
}

-> Step 3: Initialize (uses remote backend)
$ terraform init
Terraform connects to S3 / Azure Blob where Existing production state is located.

-> Step 4: Import the resource into remote state (KEY STEP). AWS/Azure examples,
$ terraform import aws_s3_bucket.prod_logs prod-logs-bucket
$ terraform import azurerm_resource_group.prod_extra \
/subscriptions/<SUB_ID>/resourceGroups/prod-rg-extra
Resource is now written into **remote state**. Safe for pipelines and team.

-> Step 5: Inspect imported state AWS/Azure
$ terraform state show aws_s3_bucket.prod_logs
$ terraform state show azurerm_resource_group.prod_extra
This shows the **real config** from cloud.

-> Step 6: Reconcile `.tf` with reality. Now running plan, shows '~ update in-place'.
$ terraform plan

Reason - Dashboard defaults ≠ Terraform defaults, Missing attributes, Tags differ, Encryption flags, policies, etc. Now fix `.tf` file until it shows 'No changes. Infrastructure is up-to-date.'.

-> Step 7: Commit and let pipeline run
$ terraform plan
$ terraform apply
The imported resource is **fully Terraform-managed**.
---

## What NOT to do in production
Don’t let pipelines run `terraform import` repeatedly
Don’t apply before import
Don’t manually edit state files
Don’t ignore dashboard-created resources long-term

---
## Recommended production rule (golden rule)
Everything in production must be either: Managed by Terraform Or explicitly documented as out-of-band.

Best teams:
* Lock down dashboards (RBAC)
* Force infra changes through PRs
* Use `terraform plan` as a gate
---

## Mental model 
.tf files  → desired state
.tfstate   → known state
Cloud      → actual state
---

????????
* Make a **ready-to-use example for both AWS and Azure pipelines** showing how to import **multiple resources automatically**.
* Show how to **detect unmanaged resources**
* Explain **drift detection tools**
* Share a **safe “import day” checklist for prod**
* Walk through a **multi-resource bulk import**
* key vault/key visibility on dashboard - add private endpoint + firewall (portal visibility changes again there)
?????????
-------------------------------------------------------------------------------------------------------------
Scenario - Suppose we are doing terraform pipeline as SP not as user and we create key vault and key, then 
we want to see the key created inside key vault > keys using dashboard as a user. 

Method 1 - (Recommended) Manual assignment of role (to enable permissions to view)
$ az role assignment create \
  --role "Key Vault Crypto Officer" \
  --assignee usrlsothdevops@gmail.com \
  --scope /subscriptions/2b2f02f7-dde2-47db-974c-47d2182721ae/resourceGroups/myTFResourceGroup/providers/Microsoft.KeyVault/vaults/prodmyappkv

# Industry best practice
# After doing this Required Microsoft Graph permission `Directory.Read.All` will not be required.

Method 2 - (Terraform Only) Use resource block for role assignment to user using its' `object_id`.

resource "azurerm_role_assignment" "human_kv_crypto_officer" {
  scope                = azurerm_key_vault.prodmyapp.id
  role_definition_name = "Key Vault Crypto Officer"
  principal_id         = "USER_OBJECT_ID_HERE"
}

# Principal - Hardcode object IDs of known humans or service accounts. Like DevOps lead or security engineer’s object IDs. No AzureAD provider block required, No Graph permissions required. It's Deterministic.
# Enables the Object id user to view the key or other resources on dashboard.

-------------------------------------------------------------------------------------------------------------
Recommended Production Pattern

(Need az cli on ec2, resources such as resource group, storage account, container (and Key) and configure the backend.tf and yaml file for Azure DevOps pipeline configuration for automation.)

--
Initial setup - 
Authenticate manually via logging-in to azure CLI and/or env vars (need to export subscription_id for
terraform) on EC2 instance. or export all SP secrets to env vars.
Create resources such as resource group, storage account, container (and Key).

--
---
Automation setup - 

Configure the backend.tf and push statefile to Azure Container. 
Create Key Vault separate for dev/test/prod.
Store secrets securely - Upload SP secrets and other credentials to Key Vault using CLI or terraform. When
uploading SP secrets using terraform available from az cli context use data block ``` data "azurerm_client_config" "current" {} ```. 
Key Vault Access Policy - Service Principal or Managed Identity with 'get' permission to access secrets.
Export client secret to env vars, as 'data.azurerm_client_config.current' cannot fetch it, and to use from env use 'var.arm_client_id' as value to key 'value'.


Configure CI/CD or automation agents to inject env vars securely or use managed identities.

-remove use_cli from backened
-Use data blocks to pull SP secrets from key vault, to be used in provider block for authentication
-From Azure DevOps dashboard, add Service Principal as a Variable Group with Key Vault integration: 
Azure DevOps Project > Pipeline > Library > Variable Groups > + Variable Group > Add Name & Description to Variable Group > Enable toggle switch button to link an Azure key vault and map selective vault secrets to this variable group > Select Azure subscription 'az-classic-conn' > Select Key vault name 'prodmyappkv' >
Variables > + Add > Select all secrets needed > Save. 

For YAML Pipeline, create and use file 'azure-pipelines.yaml' in same dir as of .tf files, push to github. Connect GitHub repo in DevOps Pipelines > New Pipeline > GitHub > Select Repository > Existing Azure Pipelines YAML file > Select YAML file. 

For Classic Pipeline ?
---
----
Future runs - 
Terraform authenticates using injected creds or managed identity, 
fetches secrets dynamically from Key Vault, and applies infrastructure changes.
----
Terraform State Security: Store Terraform state remotely (e.g., in Azure Storage with access controls), and enable state file encryption.

Sensitive Outputs: Mark any outputs of secrets or IDs as 'sensitive = true' in Terraform, or avoid outputting them altogether.
--------------------------------------------------------------------------------------------------------------
Case - Want to run terraform commands locally

- We initially ran Terraform locally, so our state was stored on the local filesystem.
- Then we moved the statefile to Azure Storage (azurerm backend) and continued creating resources.
- Now on our EC2/local Terraform, Terraform detects that the backend in your code (azurerm storage) is different from the previous local backend.

Since we’ve already moved our statefile to Azure Storage, we don’t need to migrate again. We just want Terraform to reconfigure to use the remote backend:

$ terraform init -reconfigure

This will:
- Reinitialize the Terraform working directory
- Point Terraform to the Azure Storage backend
- Keep all the existing state in the backend
- Avoid overwriting anything

or, if showing changes after -reconfigure should always follow
how to use terraform import when doing azure or aws form pipeline or locally?
terraform init -reconfigure or init > terraform import > terraform plan > terraform apply
--------------------------------------------------------------------------------------------------------------
## Rotate secrets regularly and enable alerting

A) Rotation:
	1) Manually: Periodically update the secret value in Key Vault (new SP secret/password, DB password, 		etc.). Each update creates a new secret version; update the downstream system (e.g., SP or DB) to use the new value.
	2) Automated: Use: 
		a) Event Grid + Azure Function / Logic App that listens for “SecretNearExpiry” events and 			rotates the secret (creates new value, writes to Key Vault, updates the dependent resource).
		b) For keys, use key rotation policies configured on Key Vault keys.
​
B) Alerting:
	1) Use Azure Monitor / Alerts on:
		a) Key Vault logs (audit log, secret access, secret change events).
		b) “SecretNearExpiry” events, unauthorized access attempts, or abnormal access patterns.
	2) Route alerts to email, Teams, or incident tools.

​> Secret Rotation: When rotating secrets, update both Key Vault and credentials used by automation/bots running Terraform.
-------------------------------------------------------------------------------------------------------------
*** Use artifact 'tfplan' method for production with approval gates ***

## Approval Gates - (configure in azure-pipelines.yaml)

Plan Stage → [APPROVAL GATE] → Apply Stage
         ↓                    ↓
   Creates tfplan          Deploys infrastructure

Pipeline Flow - Plan Stage > [PAUSE: Manual Approval Required] > Apply Stage 

Prevents: Accidental production deployments without review.
Pipeline stops at the gate → person approves/rejects → pipeline continues/stops.

Methods to Add Approval Gates:

Method 1: Environments (Recommended for YAML) - 
Goto Organization > Project > Pipelines > Environments > Create environment > Name 'prod' > Description 'Production environment for Terraform deployments' > Resource 'None'> Create > Approvals and checks > Approvals > Select Approver/Add email > Control options > Timeout > Create. 

*Pipeline runs > Plan completes & Pipeline pauses > Go to Environments > prod shows "Waiting for approval" > Click Approve > Apply runs

Add YAML Code under 'jobs:' as

  # ./ APPROVAL GATE via Environment
  - deployment: ProductionDeployment
    displayName: 'Deploy to Production'
    environment: 'prod'  # Create this environment in Azure DevOps first
    strategy:
      runOnce:           # standard deployment pattern
        deploy:
          steps:
          - task: TerraformInstaller@0  # Reinstall for consistency
          
          # DOWNLOAD plan artifact
          - task: DownloadPipelineArtifact@2
            inputs:
              buildType: 'current'
              artifactName: 'tfplan'
              targetPath: 'azuretf/simpletf/'


Method 2: Manual Approval Check (No environments/dashboard usage needed)
 Plan > Approval > Apply

Pipeline Flow - 
Plan completes > publishes tfplan artifact > Approval stage > **PAUSES** at "ManualValidation" > sends emails > I/team review plan >Click pipeline > "Approval stage" > click **Approve** or **Reject** buttons > if approved - Apply 

Perfect for quick production gates without environment setup.

Add YAML Code after stage plan as
- stage: Approval
  dependsOn: Plan
  condition: succeeded()
  jobs:
  - job: ManualApproval
    pool: server  # Uses Azure DevOps server (no agent needed)
    steps:
    - task: ManualValidation@0
      timeoutInMinutes: 4320  # 3 days timeout (auto-rejects after timeout)
      inputs:
        notifyUsers: |           # List of email addresses (notifications sent)
          you@company.com
          team@company.com
        instructions: |          # Custom message with plan details
          **Review the Terraform Plan above carefully.**
          
          This will deploy:
          - Resource Group: myTFResourceGroup (Australia East)
          - Storage Account: prodmyapptfstate01  
          - Storage Container: mytfstate
          - Key Vault: prodmyappkv
          
          **Approve** to proceed with deployment.
          **Reject** to stop pipeline.
          
          tfplan artifact available for download.
        onTimeout: 'reject'      # stops pipeline if no approval

- stage: Apply
  dependsOn: Approval
  condition: succeeded()
  jobs:
  - job: Apply
    steps:
    - task: TerraformInstaller@0  # Reinstall for consistency
    
    # DOWNLOAD plan artifact
    - task: DownloadPipelineArtifact@2
      inputs:
        buildType: 'current'
        artifactName: 'tfplan'
        targetPath: 'azuretf/simpletf/'

    # Add, format and use/continue locs with tasks for init and apply mentioned at end of file.
    
*** Avoid artifact 'tfplan' method and Approval Gates for dev/test ***

Plan → Apply (same workspace, no artifacts)

Commit > Push > Watch it deploy end-to-end.
Once tested successfully > add back approval gates for production safety.
Ideal for verifying your bootstrap pipeline works before adding gates.

stages:
- stage: Deploy
  jobs:
  - job: Deploy
    steps:
    - task: TerraformInstaller@0
      inputs:
        terraformVersion: '1.14.3'  # Match version

    - task: TerraformTaskV4@4
      inputs:
        provider: 'azurerm'
        command: 'init'
        workingDirectory: 'azuretf/simpletf' # since files in terraform_automation_IaC/azuretf/simpletf/
        backendServiceArm: '$(serviceConnection)'
        backendAzureRmResourceGroupName: 'myTFResourceGroup'
        backendAzureRmStorageAccountName: 'prodmyapptfstate01'
        backendAzureRmContainerName: '$(containerName)'
        backendAzureRmKey: '$(key)'
        environmentServiceNameAzureRM: '$(serviceConnection)'

    - task: TerraformTaskV4@4
      inputs:
        provider: 'azurerm'
        command: 'plan'
        workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
        environmentServiceNameAzureRM: '$(serviceConnection)'
        commandOptions: '-detailed-exitcode -var="arm_client_secret=$(ARM_CLIENT_SECRET)"'  # Optional: pass secrets

    - task: TerraformTaskV4@4
      inputs:
        provider: 'azurerm'
        command: 'apply'
        workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
        environmentServiceNameAzureRM: '$(serviceConnection)'
        commandOptions: '-auto-approve'
--------------------------------------------------------------------------------------------------------------
Defining variable for VM size selection (Production):

--------------------------------------------------------------------------------------------------------------
Scenario - We run Terraform via Azure DevOps pipelines (init → plan → apply). A person triggering or
contributing does not have Azure portal access, may not have Azure DevOps UI access (run summary), but
still needs to see Terraform outputs/logs (IPs, URLs, resource names, IDs, etc.). So, where do Terraform
outputs go, and how can one consume them?

DevOps Engineer
(Windows PC + Git Bash)
   └── Writes Terraform code
        └── git commit & push
              └── GitHub repository
                    └── Azure DevOps Pipeline (YAML)
                          └── Self-hosted Agent
                                └── Linux VM on AWS EC2
                                      └── terraform init / plan / apply




XXXXXX -----            ---------XXXXXX
“full implementation for logs branch + commit comment”




XXXXXX -----            ---------XXXXXX
--------------------------------------------------------------------------------------------------------------

============================================================
TERRAFORM ENTERPRISE PIPELINE FIX SUMMARY
=========================================

1. Increase Terraform Lock Timeout

---

Problem:
State lock errors can occur if another process is using the backend.

Fix:
Increase lock timeout from 5 minutes to 10 minutes.

Update runterraform.sh:

Change:
-lock-timeout=5m

To:
-lock-timeout=10m

Apply to:

* terraform init
* terraform plan
* terraform apply

============================================================

2. Add Backend Pre-Check Before Plan

---

Problem:
Plan fails if backend is locked or unreachable.

Fix:
Add state access check before running plan.

Add inside runterraform.sh (plan section):

echo "Pre-check: verifying backend state access..."
terraform state pull > /dev/null 2>&1

if [[ $? -ne 0 ]]; then
echo "ERROR: Cannot access remote state. Backend may be locked."
TF_EXIT=1
else
terraform plan 
-no-color 
-detailed-exitcode 
-lock-timeout=10m 
-out=tfplan.binary
fi

This prevents unsafe force-unlock and fails cleanly.

============================================================

3. Prevent Concurrent Pipelines (Enterprise Fix)

---

Problem:
Two pipelines running at same time cause state lock errors.

Fix:
Use Azure DevOps Deployment Job + Environment Lock.

Update YAML job type:

Replace:

* - job: Terraform

With:

* - deployment: Terraform

Add environment:

environment: "terraform-prod"

Move pool inside deployment job:

pool:
name: 'az-classic-pool'

Full structure:

* - deployment: Terraform
    environment: "terraform-prod"
    pool:
      name: 'az-classic-pool'
    strategy:
      runOnce:
        deploy:
          steps:
          (existing steps)

============================================================

4. Create Environment in Azure DevOps

---

Go to:
Pipelines → Environments

Create new environment:
terraform-prod

Then:
Open environment → Security
Grant Build Service access
OR re-run pipeline and click "Authorize"

============================================================

5. Enable Exclusive Lock (Recommended)

---

Open:
Pipelines → Environments → terraform-prod

Go to:
Approvals and checks

Add:
Exclusive lock

Result:
Only ONE pipeline can deploy at a time.
Prevents state corruption.

============================================================

## FINAL RESULT

✔ Increased lock timeout (10m)
✔ Backend pre-check before plan
✔ Deployment job instead of normal job
✔ Environment created and authorized
✔ Exclusive lock enabled
✔ No unsafe force-unlock required
✔ Enterprise-grade Terraform pipeline

============================================================

# END OF SUMMARY

--------------------------------------------------------------------------------------------------------------
Terraform SSH Key Creation and Use with VM afterwards :

Creation part:
- With the use of tls provider settings. 
We create SSH key first with 
resource "tls_private_key" "vm_ssh" {...} block

- With the use of local provider settings.
We write private and public files with permissions for use, on agent VM with
resource "local_sensitive_file" "private_key_pem" {...} and,
resource "local_file" "public_key_openssh" {...} blocks.

[ Agent Server ]
  ~/.ssh/prodmyapp-vm.pem   (PRIVATE KEY)
          |
          |  ssh -i prodmyapp-vm.pem
          v
[ Azure VM ]
  ~/.ssh/authorized_keys   (PUBLIC KEY)

How to SSH into the VM after apply from agent server/VM:
$ ssh -i ~/.ssh/prodmyapp-vm.pem adminuser@<PUBLIC_IP>

- We SSH from the az agent server / jump VM into the created Azure VM
- Key files exist only on the agent server
- The private key is NOT copied to the Azure VM
- The public key is injected into the VM by Azure during creation

---------------------------------------------------------------------------------------------------------------
MEMORY ERROR HANDLING/TROUBLESHOOTING
------------------------------x

Terraform error exit codes - 
Exit 0 - Success
Exit 0 - Failure (script still completes)
Exit 1 - Crash (failure + summary) 

xxxxxxxxxx Scenario 2 
EC2 memory exhaustion issue (Exit code 137 = OOM killer) - Out of memory (OOM) - Agent crash.
Reason - heavy artifact publishing upload size

Error 1 - 
```
Starting: PublishPipelineArtifact
==============================================================================
Task         : Publish Pipeline Artifacts
Description  : Publish (upload) a file or directory as a named artifact for the current run
Version      : 1.242.0
Author       : Microsoft Corporation
Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/publish-pipeline-artifact
==============================================================================
Artifact name input: terraform-logs
Uploading pipeline artifact from /opt/agent/_work/1/s/azuretf/simpletf for build #110
Using default max parallelism.
Max dedup parallelism: 192
DomainId: 0
ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session ebc40439-e747-4c22-9fcf-73c1eeba487c
Hashtype: Dedup1024K
DedupManifestArtifactClient will correlate http requests with X-TFS-Session ebc40439-e747-4c22-9fcf-73c1eeba487c
18 files processed.
Processed 18 files from /opt/agent/_work/1/s/azuretf/simpletf successfully.
Uploading 18 files from directory /opt/agent/_work/1/s/azuretf/simpletf.
Uploaded 71,399,829 out of 303,696,955 bytes.
##[warning]Free memory is lower than 5%; Currently used: 99.73%
##[warning]Free memory is lower than 5%; Currently used: 100.00%
##[warning]Free memory is lower than 5%; Currently used: 100.00%
##[warning]Free memory is lower than 5%; Currently used: 100.00%
Uploading 18 files from directory /opt/agent/_work/1/s/azuretf/simpletf.
Uploaded 71,399,829 out of 303,696,955 bytes.
##[error]Exit code 137 returned from process: file name '/opt/agent/bin/Agent.PluginHost', arguments 'task "Agent.Plugins.PipelineArtifact.PublishPipelineArtifactTaskV1, Agent.Plugins"'.
Finishing: PublishPipelineArtifact

```

Error 2 - script didn't ran and exited with non zero 
``` 
Starting: Terraform Init → GitHub Checks
==============================================================================
Task         : Command line
Description  : Run a command line script using Bash on Linux and macOS and cmd.exe on Windows
Version      : 2.268.0
Author       : Microsoft Corporation
Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/command-line
==============================================================================
Generating script.
Script contents:
./RunTerraform.sh init
========================== Starting Command Output ===========================
/usr/bin/bash --noprofile --norc /opt/agent/_work/_temp/590913ab-7cdb-4928-a6b2-22a29c2e37a7.sh
Running terraform init...

##[error]Bash exited with code '1'.
Finishing: Terraform Init → GitHub Checks

```

Solutions/Handling - 
-----------------------

1) Limit artifact parallelism / Limit dedup parallelism:
Add this to yaml code 
```
env:
  AZURE_DEVOPS_DEDUP_PARALLELISM: "1"
``` 
Example:

    # -----------------------------
    # Publish logs only
    # -----------------------------
    - task: PublishPipelineArtifact@1
      condition: failed()             
      # always() - optional
      inputs:
        targetPath: artifacts
        artifact: terraform-logs
      env:
        AZURE_DEVOPS_DEDUP_PARALLELISM: "1"

```
This affects only,
- PublishPipelineArtifact
- Azure DevOps deduplication engine
- Upload chunk hashing
- Network concurrency
and 
- Fixes artifact upload OOM
- Does nothing for Terraform

2) Limit Terraform parallelism (-parallelism): 
Preventing memory spike by Stabilizing low-memory agents. 
Add to shell script (top) or export directly to env using bash cli, 
```
export TF_CLI_ARGS_plan="-parallelism=5"
export TF_CLI_ARGS_apply="-parallelism=5"
```
This affects,
- Provider plugin execution
- Resource graph evaluation
- Concurrent API calls
- In-memory state graph expansion
and,
- NOT controlled by Azure DevOps
- NOT affected by dedup settings
because,
Terraform runs inside our script, not inside the artifact task.

Difference b/w artifact / dedup parallelism and Terraform parallelism (this is deep CI/CD
engineering territory).

2) Reduce Terraform init memory usage by Preventing repeated provider downloads, Reducing RAM spikes and
by Avoiding unzip storms.
Add this before terraform init into RunTerraform.sh,
```
export TF_PLUGIN_CACHE_DIR="$PWD/.terraform-plugin-cache"
mkdir -p "$TF_PLUGIN_CACHE_DIR"
```
Then change/use init as,
```
terraform init -no-color -plugin-dir="$TF_PLUGIN_CACHE_DIR" > "$LOG_FILE" 2>&1 || true
```

2) Add swap - EC2-side fixes (strongly recommended)
Run on ec2,
$ sudo fallocate -l 4G /swapfile
$ sudo chmod 600 /swapfile
$ sudo mkswap /swapfile
$ sudo swapon /swapfile

Persist - making swap memory permanent across reboots, 
$ echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
# Every time the EC2 instance boots, enable /swapfile as swap memory.

* /swapfile - This is the path to your swap file
* none - Swap does not get mounted to a directory (unlike disks), So none is used instead of a mount point
* swap - Filesystem type = swap, Tells Linux this entry is for swap space
* sw - Mount options, sw = enable swap with default options
* 0 - Dump field, Swap does not need filesystem backups so 0
* 0 - fsck order, Swap should not be checked at boot so 0
* /etc/fstab = filesystem table, tells Linux to mount disks/filesystems/swap automatically at boot. 

Verify swap 
$ swapon --show
$ free -h

3) Minimum EC2 sizing (realistic) - t3.medium (4 GB)/t3.large 

4) Drop Linux page cache (only if memory is low), When free memory is critically low. 
Without killing process, agent running as user with sudo.
---
Example:
- script: |
    FREE=$(free | awk '/Mem:/ {print int($4/$2*100)}')
    if [ "$FREE" -lt 10 ]; then
      echo "Low memory detected ($FREE%), clearing caches"
      sudo sync
      echo 3 | sudo tee /proc/sys/vm/drop_caches
    else
      echo "Memory OK ($FREE%), skipping cache clear"
    fi
  condition: always()
  displayName: "Conditional memory cleanup"
---

5) Restart agent nightly (best practice) - Long-running agents accumulate memory fragmentation.
Create a cron job:
sudo crontab -e
Add:
0 3 * * * systemctl restart azure-pipelines-agent

STORAGE SOLUTIONS:
------------------
6) EC2/VM Disk Occupied by,
- Agent work folders - `/opt/agent/_work/` - old pipeline runs not deleted automatically.
- Terraform leftovers - `.terraform/`, `*.tfstate*`, `terraform providers`.
- Pipeline artifacts cache - `/opt/agent/_work/_tool/`.
- Docker images.
- Old logs + temp files. 

SAFE cleanup strategy:

a) Check disk usage before/after, run - 
$ df -h
$ du -sh /opt/agent/_work/*

b) Clean old pipeline work directories as Azure DevOps does NOT auto-clean old runs. Safe rule is to
keep last 3 runs delete the rest. This frees GBs of space safely, run this on the EC2 agent:
# Go to directory
$ cd /opt/agent/_work

# List run directories
ls -d [0-9]*/ 2>/dev/null

# Keep last 3, delete the rest
ls -dt [0-9]*/ | tail -n +4 | xargs -r rm -rf

c) Clean agent temp folder - Safe anytime — files are recreated automatically.
$ rm -rf /opt/agent/_work/_temp/*

d) Clean tool cache (optional, but large) - Terraform installers accumulate here. TerraformInstaller
task will re-download automatically, to clean run:
$ rm -rf /opt/agent/_work/_tool/terraform

f) AUTOMATE THE PROCESS - Create a safe cleanup script:
create file - `/opt/agent/cleanup-agent.sh`, add code below -

```
#!/bin/bash
set -e

echo "Cleaning Azure DevOps agent storage..."

cd /opt/agent/_work || exit 0

# Keep last 3 pipeline runs
ls -dt [0-9]*/ 2>/dev/null | tail -n +4 | xargs -r rm -rf

# Clean temp
rm -rf /opt/agent/_work/_temp/*

# Optional: clean terraform tools
rm -rf /opt/agent/_work/_tool/terraform

echo "Cleanup complete"
df -h
```

Make script executable:
$ chmod +x /opt/agent/cleanup-agent.sh

Run `cleanup-agent.sh` during night via cron as
$ sudo crontab -e
add,
$ 0 2 * * * /opt/agent/cleanup-agent.sh >> /var/log/agent-cleanup.log 2>&1

e) Clean Terraform junk after every run
Prevent memory + disk bloat across runs.
run/use,
```
- script: |
    rm -rf azuretf/simpletf/.terraform
    rm -f azuretf/simpletf/.terraform.lock.hcl
    rm -f *.tfstate*
    rm -f azuretf/simpletf/tfplan.binary
  condition: always()
  displayName: "Cleanup Terraform working directory/Pipeline cleanup"
```
Or/And,
Clean Terraform junk (repo-level) - Inside your repo directory run:
$ find . -type d -name ".terraform" -prune -exec rm -rf {} \;
$ find . -type f -name "*.tfstate*" -delete
$ find . -type f -name "tfplan.binary" -delete

f) After doing above cleanups if getting error like job is marked “abandoned”, 
Perform Safe cleanup on EC2 host - 
sudo systemctl stop vsts.agent*
rm -rf /opt/agent/_work/1/*
rm -rf /opt/agent/_work/_temp/*
rm -rf /opt/agent/_work/_tasks/*
sudo systemctl start vsts.agent*
then continue with ./run.sh of agent.

#
This is exactly how large enterprises run Terraform on self-hosted agents:
- Swap enabled
-  AZURE_DEVOPS_DEDUP_PARALLELISM=1
- TF_CLI_ARGS_plan="-parallelism=5"
- TF_CLI_ARGS_apply="-parallelism=5"
- Plugin cache enabled
- Minimal artifacts
#
xxxxxxxxxx 

--------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------
Verify GitHub Token from EC2/VM:
```
curl -i \
  -H "Authorization: Bearer ghp_k7pt3nlRS6nYdQlZxxxjcSjJpL02CN1rCmwl" \
  -H "Accept: application/vnd.github+json" \
  https://api.github.com/user
```
Or,
```
curl -H "Authorization: Bearer YOUR_TOKEN" https://api.github.com/user
```
---------------------------------------------------------------------------------------------------------------
Production Deployment Checklist:

- Separate state files per environment
- Compiled plans (plan → artifact → apply)
- Environment-specific SPs with scoped RBAC  
- Manual approval gates for prod
- GitHub Releases for outputs (primary access)
- Key Vault backup (secondary access)
- Slack/Teams notifications
- terraform fmt/validate in PR checks
- Drift detection (plan on schedule)
- State file versioning (keep 5 backups)
- Multi-agent auto-scaling
- Immutable agent AMI
- Spot instance + recovery
- One-click agent bootstrap
- Calculate the optimal parallelism for your EC2 size
- Show how to auto-detect RAM and set parallelism dynamically
- Provide a hardened RunTerraform.sh v3

-----------------
git branch -r
git branch -a 
git branch -all
-----------------
# ------------------------------------------------------------------------------------------------------------
# Install jq - command-line JSON processor (required for GitHub Checks)
# -----------------------------
#- script: |
#    if command -v jq >/dev/null 2>&1; then
#      echo "jq already installed"
#      exit 0
#    fi

#    echo "Installing jq..."

#    if command -v yum >/dev/null 2>&1; then
#      sudo yum install -y jq
#    elif command -v apt-get >/dev/null 2>&1; then
#      sudo apt-get update
#      sudo apt-get install -y jq
#    else
#      echo "No supported package manager found"
#      exit 1
#    fi
#  displayName: "Install jq"
      
#** Use `sudo yum install -y jq` once on ec2/VM and comment above code or 
# keep using above code to install jq (OPTIONAL)
# ------------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------------
# Clean logs
# -----------------------------
#- script: |
#    rm -f azuretf/simpletf/*.log azuretf/simpletf/annotations.json
#  displayName: "Clean previous logs"
# -------------------------------------------------------------------------------------------------------------
PIPELINE CODE:
#---

# When access to azure devops pipeline jobs run summary available

#trigger:
#- main

#variables:
#  serviceConnection: 'az-classic-conn'
#  containerName: 'mytfstate'
#  key: 'terraform.tfstate'
#  #resourceGroup: 'myTFResourceGroup'

#pool:
#  name: 'az-classic-pool'  # EC2 self-hosted agent pool


############# Avoid Artifact 'tfplan' and Approval Gate - works for dev/test using '-auto-approve'

#stages:
#- stage: Deploy
#  jobs:
#  - job: Deploy
#    steps:
#    - task: TerraformInstaller@0
#      inputs:
#        terraformVersion: '1.14.3'  # Match version

#    - task: TerraformTaskV4@4
#      inputs:
#        provider: 'azurerm'
#        command: 'init'
#        workingDirectory: 'azuretf/simpletf' # since files in terraform_automation_IaC/azuretf/simpletf/
#        backendServiceArm: '$(serviceConnection)'
#        backendAzureRmResourceGroupName: 'myTFResourceGroup'
#        backendAzureRmStorageAccountName: 'prodmyapptfstate01'
#        backendAzureRmContainerName: '$(containerName)'
#        backendAzureRmKey: '$(key)'
#        environmentServiceNameAzureRM: '$(serviceConnection)'
#        #commandOptions: '-upgrade'

#    - task: TerraformTaskV4@4
#      name: TerraformPlan
#      inputs:
#        provider: 'azurerm'
#        command: 'plan'
#        workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
#        environmentServiceNameAzureRM: '$(serviceConnection)'
#        commandOptions: >
#          -no-color                                           
#          -var="arm_client_secret=$(ARM_CLIENT_SECRET)"       
        
        # `-no-color` - makes logs readable outside UI
        # `-var="<arm-client-secret>"` - Can also pass secret directly

#    - task: TerraformTaskV4@4
#      inputs:
#        provider: 'azurerm'
#        command: 'apply'
#        workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
#        environmentServiceNameAzureRM: '$(serviceConnection)'
#        commandOptions: >
#          -no-color
#          -auto-approve                              
          
        # `-auto-approve` - no approval required dev/stage


############ Use Artifact 'tfplan' and Approval Gate
#stages:
#- stage: Plan
#  jobs:
#  - job: Plan
#    steps:
#    - task: TerraformInstaller@0
#      inputs:
#        terraformVersion: '1.14.3'  # Match version

#    - task: TerraformTaskV4@4  # Or V2/V3 based on marketplace
#      inputs:
#        provider: 'azurerm'
#        command: 'init'
#        workingDirectory: 'azuretf/simpletf' # since files in terraform_automation_IaC/azuretf/simpletf/
#        backendServiceArm: '$(serviceConnection)'
#        backendAzureRmResourceGroupName: 'myTFResourceGroup'
#        backendAzureRmStorageAccountName: 'prodmyapptfstate01'
#        backendAzureRmContainerName: '$(containerName)'
#        backendAzureRmKey: '$(key)'
#        environmentServiceNameAzureRM: '$(serviceConnection)'

#    - task: TerraformTaskV4@4
#      inputs:
#        provider: 'azurerm'
#        command: 'plan'
#        workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
#        environmentServiceNameAzureRM: '$(serviceConnection)'
#        commandOptions: '-out=tfplan'
        
    # PUBLISH plan as artifact
#    - task: PublishPipelineArtifact@1
#      inputs:
#        targetPath: 'azuretf/simpletf/tfplan'
#        artifact: 'tfplan'
#        publishLocation: 'pipeline'

####### Approval Gate - Method 1: Environments
#- stage: Apply
#  dependsOn: Plan
#  condition: succeeded()
#  jobs:
  
    # APPROVAL GATE via Environment
#  - deployment: ProductionDeployment
#    displayName: 'Deploy to Production'
#    environment: 'prod'  # Create this environment in Azure DevOps first as it triggers approval gate
#    strategy:
#      runOnce:           # standard deployment pattern
#        deploy:
#          steps:
#          - task: TerraformInstaller@0  # Reinstall for consistency
          
          # DOWNLOAD plan artifact
#          - task: DownloadPipelineArtifact@2
#            inputs:
#              buildType: 'current'
#              artifactName: 'tfplan'
#              targetPath: 'azuretf/simpletf/'
          
          # Add, format and use/continue locs with tasks for init and apply mentioned at end of file.  
                    
####### Approval Gate - Method 2: Manual Approval Check (No environments needed)
#- stage: Approval
#  dependsOn: Plan
#  condition: succeeded()
#  jobs:
#  - job: ManualApproval
#    pool: server  # Uses Azure DevOps server (no agent needed)
#    steps:
#    - task: ManualValidation@0
#      timeoutInMinutes: 4320  # 3 days timeout
#      inputs:
#        notifyUsers: |
#          you@company.com
#          team@company.com
#        instructions: |
#          **Review the Terraform Plan above carefully.**
          
#          This will deploy:
#          - Resource Group: myTFResourceGroup (Australia East)
#          - Storage Account: prodmyapptfstate01  
#          - Storage Container: mytfstate
#          - Key Vault: prodmyappkv
          
#          **Approve** to proceed with deployment.
#          **Reject** to stop pipeline.
          
#          tfplan artifact available for download.
#        onTimeout: 'reject'

#- stage: Apply
#  dependsOn: Approval
#  condition: succeeded()
#  jobs:
#  - job: Apply
#    steps:
#    - task: TerraformInstaller@0  # Reinstall for consistency
    
    # DOWNLOAD plan artifact
#    - task: DownloadPipelineArtifact@2
#      inputs:
#        buildType: 'current'
#        artifactName: 'tfplan'
#        targetPath: 'azuretf/simpletf/'

    # Add, format and use/continue locs with tasks for init and apply mentioned at end of file.

########
  
#          - task: TerraformTaskV4@4
#            inputs:
#              provider: 'azurerm'
#              command: 'init'
#              workingDirectory: 'azuretf/simpletf'  # since files in terraform_automation_IaC/azuretf/simpletf/
#              backendServiceArm: '$(serviceConnection)'
#              backendAzureRmResourceGroupName: 'myTFResourceGroup'
#              backendAzureRmStorageAccountName: 'prodmyapptfstate01'
#              backendAzureRmContainerName: '$(containerName)'
#              backendAzureRmKey: '$(key)'
#              environmentServiceNameAzureRM: '$(serviceConnection)'

#          - task: TerraformTaskV4@4
#            inputs:
#              provider: 'azurerm'
#              command: 'apply'
#              workingDirectory: 'azuretf/simpletf'   # since files in terraform_automation_IaC/azuretf/simpletf/
#              environmentServiceNameAzureRM: '$(serviceConnection)'
#              commandOptions: 'tfplan'
# -------------------------------------------------------------------------------------------------------------

